{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import yaml\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16063727183c42628558f86bf970e2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482fc68ed8a540b19fb59075c155cf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc1e24d3b2f482b81ee2bc088a9ad42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"torch_mlp_config.yaml\")\n",
    "\n",
    "mlp_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"mlp_weights.pt\")\n",
    "\n",
    "wavenet_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"wavenet_config.yaml\")\n",
    "\n",
    "wavenet_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"wavenet_weights.pt\")\n",
    "\n",
    "gpt_micro_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"micro_gpt_config.yaml\")\n",
    "\n",
    "gpt_micro_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"micro_gpt_weights.pt\")\n",
    "\n",
    "gpt_rev_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"rev_gpt_config.yaml\")\n",
    "\n",
    "gpt_rev_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"rev_gpt_weights.pt\")\n",
    "\n",
    "gpt_first_rev_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"first_name_gpt_config.yaml\")\n",
    "\n",
    "gpt_first_rev_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"first_name_gpt_weights.pt\")\n",
    "\n",
    "with open(mlp_config_path, 'r') as file:\n",
    "    mlp_config = yaml.safe_load(file)\n",
    "\n",
    "with open(wavenet_config_path, 'r') as file:\n",
    "    wavenet_config = yaml.safe_load(file)\n",
    "    \n",
    "with open(gpt_micro_config_path, 'r') as file:\n",
    "    gpt_micro_config = yaml.safe_load(file)\n",
    "\n",
    "with open(gpt_rev_config_path, 'r') as file:\n",
    "    gpt_rev_config = yaml.safe_load(file)\n",
    "\n",
    "with open(gpt_first_rev_config_path, 'r') as file:\n",
    "    gpt_first_rev_config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (first): Linear(in_features=30, out_features=200, bias=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (4): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (7): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Tanh()\n",
       "    (9): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (10): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Tanh()\n",
       "    (12): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (13): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Tanh()\n",
       "    (15): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (16): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): Tanh()\n",
       "    (18): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (19): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): Tanh()\n",
       "    (21): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (22): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): Tanh()\n",
       "    (24): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (25): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Tanh()\n",
       "    (27): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (28): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Tanh()\n",
       "  )\n",
       "  (final): Linear(in_features=200, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "## MLP\n",
    "##################################################################################\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_char, hidden_nodes, embeddings, window, num_layers):   \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.window = window\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.C = nn.Parameter(torch.randn((num_char, embeddings)) * 0.1, requires_grad=True)\n",
    "        \n",
    "        self.first = nn.Linear(embeddings*window, hidden_nodes)\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.layers = self.layers.extend(nn.Sequential(\n",
    "                nn.Linear(hidden_nodes, hidden_nodes, bias=False),\n",
    "                nn.BatchNorm1d(hidden_nodes),\n",
    "                nn.Tanh()))\n",
    "\n",
    "        self.final = nn.Linear(hidden_nodes, num_char)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.C[x]\n",
    "        x = self.first(x.view(-1, self.window*self.embeddings))\n",
    "        \n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "    \n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "    \n",
    "mlp = MLP(mlp_config['num_char'], \n",
    "          mlp_config['hidden_nodes'], \n",
    "          mlp_config['embeddings'], \n",
    "          mlp_config['window'], \n",
    "          mlp_config['num_layers'])\n",
    "\n",
    "mlp.load_state_dict(torch.load(mlp_weights_path))\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Embedding(30, 10)\n",
       "    (1): Conv1d(8, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Tanh()\n",
       "    (4): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "    (7): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (8): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Tanh()\n",
       "    (10): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (11): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Tanh()\n",
       "    (13): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (14): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Tanh()\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=500, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "## WaveNet\n",
    "##################################################################################\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, num_char, hidden_nodes, embeddings, window, num_layers):   \n",
    "        super(WaveNet, self).__init__()\n",
    "        \n",
    "        self.window = window\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.embeddings = embeddings        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Embedding(num_char, embeddings)\n",
    "        )\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                nodes = window\n",
    "            else:\n",
    "                nodes = hidden_nodes\n",
    "                \n",
    "            self.layers = self.layers.extend(nn.Sequential(\n",
    "                nn.Conv1d(nodes, hidden_nodes, kernel_size=2, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(hidden_nodes),\n",
    "                nn.Tanh()))\n",
    "            \n",
    "        self.layers = self.layers.extend(nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_nodes*(embeddings-num_layers), num_char)\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "    \n",
    "wavenet = WaveNet(wavenet_config['num_char'], \n",
    "                  wavenet_config['hidden_nodes'], \n",
    "                  wavenet_config['embeddings'], \n",
    "                  wavenet_config['window'], \n",
    "                  wavenet_config['num_layers'])\n",
    "wavenet.load_state_dict(torch.load(wavenet_weights_path))\n",
    "wavenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (vocab_emb): Embedding(27, 128)\n",
       "  (pos_emb): Embedding(32, 128)\n",
       "  (emb_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (head): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (head): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (head): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (head): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (head): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (head): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "## Transformer\n",
    "##################################################################################\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class GptAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    For this attention module k = v = q are all the same.\n",
    "    It's for encoder/decoder only transfomers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(GptAttention, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config[\"d_model\"] % self.config[\"heads\"] == 0\n",
    "        self.heads = self.config[\"heads\"]\n",
    "\n",
    "        self.w_attn = nn.Linear(self.config[\"d_model\"], 3*self.config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"d_model\"])\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config[\"attn_pdrop\"])\n",
    "        self.resid_dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\", \n",
    "            torch.tril(\n",
    "                torch.ones(\n",
    "                    self.config[\"window\"], \n",
    "                    self.config[\"window\"])\n",
    "                ).view(1, 1, self.config[\"window\"], self.config[\"window\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, window, embs = x.shape\n",
    "\n",
    "        q, v, k = self.w_attn(x).split(self.config[\"d_model\"], dim=2)\n",
    "\n",
    "        # (B, heads, window, embs)\n",
    "        q = q.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        k = k.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        v = v.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # Self-attend: (B, heads, window, embs) x (B, heads, embs, window) -> (B, heads, window, window)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        mask = scores.masked_fill(self.bias[:,:,:window,:window] == 0, float('-inf'))\n",
    "        probs = F.softmax(mask, dim=-1)\n",
    "        attn = self.attn_dropout(probs)\n",
    "        attn = probs @ v\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, window, embs)\n",
    "\n",
    "        return self.resid_dropout(self.head(attn))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(config[\"d_model\"], 4*config[\"d_model\"])\n",
    "        self.l2 = nn.Linear(4*config[\"d_model\"], config[\"d_model\"])\n",
    "        self.dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = NewGELU()(self.l1(x))\n",
    "        return self.dropout(self.l2(x))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.attn = GptAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm2 = nn.LayerNorm(config[\"d_model\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.vocab_emb = nn.Embedding(self.config[\"vocab\"], self.config[\"d_model\"])\n",
    "        self.pos_emb = nn.Embedding(self.config[\"window\"], self.config[\"d_model\"])\n",
    "        self.emb_dropout = nn.Dropout(config[\"embd_pdrop\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(self.config) for _ in range(self.config[\"blocks\"])])\n",
    "        self.head_layer_norm = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"vocab\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        vocab_emb = self.vocab_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(0, x.shape[1], dtype=torch.long, device=x.device))\n",
    "\n",
    "        x = self.emb_dropout(vocab_emb + pos_emb)\n",
    "\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "\n",
    "        x = self.head_layer_norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_opt(self):\n",
    "        p_decay = set()\n",
    "        p_no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    p_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = p_decay & p_no_decay\n",
    "        union_params = p_decay | p_no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_decay))], \"weight_decay\": self.config[\"weight_decay\"]},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, \n",
    "            lr=self.config[\"lr\"], \n",
    "            betas=(self.config[\"b1\"], self.config[\"b2\"])\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "gpt_micro = GPT(gpt_micro_config)\n",
    "gpt_micro.load_state_dict(torch.load(gpt_micro_weights_path))\n",
    "gpt_micro.eval()\n",
    "\n",
    "gpt_rev = GPT(gpt_rev_config)\n",
    "gpt_rev.load_state_dict(torch.load(gpt_rev_weights_path))\n",
    "gpt_rev.eval()\n",
    "\n",
    "gpt_first_rev = GPT(gpt_first_rev_config)\n",
    "gpt_first_rev.load_state_dict(torch.load(gpt_first_rev_weights_path))\n",
    "gpt_first_rev.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "## Gradio App\n",
    "##################################################################################\n",
    "\n",
    "def generate_names(name_start, name_end, number_of_names, model):\n",
    "    if number_of_names < 0:\n",
    "        return \"Error: Please enter a positive number of names to generate!\"\n",
    "        \n",
    "    # Select model\n",
    "    if model == \"MLP\":\n",
    "        config = mlp_config\n",
    "        sample_fcn = mlp.sample_char\n",
    "    elif model == \"WaveNet\":\n",
    "        config = wavenet_config\n",
    "        sample_fcn = wavenet.sample_char\n",
    "    elif model == \"GPT Micro\":\n",
    "        config = gpt_micro_config\n",
    "        sample_fcn = gpt_micro.sample_char\n",
    "    elif model == \"GPT Rev\":\n",
    "        config = gpt_rev_config\n",
    "        sample_fcn = gpt_rev.sample_char\n",
    "    elif model == \"GPT First Rev\":\n",
    "        config = gpt_first_rev_config\n",
    "        sample_fcn = gpt_first_rev.sample_char\n",
    "    else:\n",
    "        return \"Error: Model not selected\"\n",
    "\n",
    "    stoi = config['stoi']\n",
    "    itos = {s:i for i,s in stoi.items()}\n",
    "\n",
    "    output = \"\"\n",
    "\n",
    "    # Sanitize user inputs, and append errors to output\n",
    "    name_end = name_end.lower()\n",
    "    name_start = name_start.lower()\n",
    "\n",
    "    for c in name_end:\n",
    "        if c not in stoi:\n",
    "            return \"Please check in name end. \\\"\" + c + \"\\\" not included in the training set.\"\n",
    "\n",
    "    for c in name_start:\n",
    "        if c not in stoi:\n",
    "            return \"Please check name start. \\\"\" + c + \"\\\" not included in the training set.\"\n",
    "\n",
    "    if \"num_final_chars_in_dataset\" in config and len(name_end) > config[\"num_final_chars_in_dataset\"]:\n",
    "        name_end = name_end[-config[\"num_final_chars_in_dataset\"]:]\n",
    "        output += \"Only accepts up to \" + str(config[\"num_final_chars_in_dataset\"]) + \" final chars. Using: \" + str(name_end) + \"\\n\"\n",
    "            \n",
    "    elif \"num_final_chars_in_dataset\" not in config and name_end != \"\":\n",
    "        output += \"Final chars not used. Need to use a \\\"Rev\\\" model trained with this feature.\\n\"\n",
    "\n",
    "\n",
    "    ## Print requested names\n",
    "    for _ in range((int)(number_of_names)):\n",
    "        name = \"\"\n",
    "        context = [0] * config['window']\n",
    "        \n",
    "        if \"num_final_chars_in_dataset\" in config:\n",
    "            for c in name_end:\n",
    "                context = context[1:] + [stoi[c]]\n",
    "            context = context[1:] + [stoi['.']]\n",
    "    \n",
    "        # Initialize name with user input\n",
    "        for c in name_start:\n",
    "            name += c\n",
    "            context = context[1:] + [stoi[c]]\n",
    "\n",
    "        # Run inference to finish off the name\n",
    "        while True:\n",
    "            x = torch.tensor(context).view(1, -1)\n",
    "            ix = sample_fcn(x)\n",
    "                \n",
    "            context = context[1:] + [ix]\n",
    "            name += itos[ix]\n",
    "                \n",
    "            if ix == 0:\n",
    "                break\n",
    "            \n",
    "        output += name + \"\\n\"\n",
    "        \n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_names,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Start name with...\"),\n",
    "        gr.Textbox(placeholder=\"End name with... (only works for rev model)\"),\n",
    "        gr.Number(value=5),\n",
    "        gr.Dropdown([\"MLP\", \"WaveNet\", \"GPT Micro\", \"GPT Rev\", \"GPT First Rev\"], value=\"GPT Rev\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    ")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
