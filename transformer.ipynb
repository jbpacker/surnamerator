{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49678a4-3895-486c-a878-18b53d2f9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import lovely_tensors as lt\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "m1 = torch.device(\"mps\")\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b1a67-473d-4496-bf2e-fa96a47c2e57",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Super helpful walkthrough: http://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c93796-5f49-43c7-a272-99bf23dc04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars in dictionary\n",
    "vocab = 30\n",
    "\n",
    "# d_model is the same as embedding same for simplicity.\n",
    "# embs = 12\n",
    "d_model = 12\n",
    "\n",
    "# number of chars to see in one window\n",
    "window = 16\n",
    "\n",
    "# This will increase one day\n",
    "batch_size = 64\n",
    "\n",
    "heads = 3\n",
    "\n",
    "blocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc54ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = torch.randint(vocab, (batch_size, window))\n",
    "prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed108a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    For this attention module k = v = q are all the same.\n",
    "    It's for encoder only transfomers.\n",
    "    \"\"\"\n",
    "    def __init__(self, heads, d_model):\n",
    "        super(GptAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.heads = heads\n",
    "\n",
    "        self.w_attn = nn.Linear(d_model, 3*d_model)\n",
    "        self.head = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(window, window))\n",
    "                                     .view(1, 1, window, window))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, window, embs = x.shape\n",
    "\n",
    "        q, v, k = self.w_attn(x).split(d_model, dim=2)\n",
    "\n",
    "        # (B, heads, window, embs)\n",
    "        q = q.view(B, window, self.heads, embs // self.heads).transpose(1, 2)\n",
    "        k = k.view(B, window, self.heads, embs // self.heads).transpose(1, 2)\n",
    "        v = v.view(B, window, self.heads, embs // self.heads).transpose(1, 2)\n",
    "        \n",
    "        # Self-attend: (B, heads, window, embs) x (B, heads, embs, window) -> (B, heads, window, window)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        mask = scores.masked_fill(self.bias[:,:,:window,:window] == 0, float('-inf'))\n",
    "        probs = F.softmax(mask, dim=-1)\n",
    "        attn = probs @ v\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, window, embs)\n",
    "\n",
    "        return self.head(attn)\n",
    "\n",
    "# gpt_attn = GptAttention(heads, d_model)\n",
    "# out = gpt_attn(enc_prompt)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba13ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, 2*d_model)\n",
    "        self.l2 = nn.Linear(2*d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        return self.l2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1521db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(Block, self).__init__()\n",
    "        self.attn = GptAttention(heads, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.attn(x)\n",
    "        x = self.norm1(x + attn)\n",
    "        ff = self.ff(x)\n",
    "        x = self.norm2(x + ff)\n",
    "        return x\n",
    "\n",
    "# b = Block(d_model, 3)\n",
    "# out = b(emb_prompts)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a13c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, d_model, heads, blocks):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_emb = nn.Embedding(vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(window, d_model)\n",
    "        # self.vocab_emb = Embeddings(d_model, vocab)\n",
    "        # self.pos_emb = PositionalEncoding(d_model, window)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(d_model, heads) for _ in range(blocks)])\n",
    "        self.head = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        vocab_emb = self.vocab_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(0, x.shape[1]).to(m1))\n",
    "\n",
    "        x = vocab_emb + pos_emb\n",
    "\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "\n",
    "# gpt = GPT(d_model, heads, blocks)\n",
    "\n",
    "# X, Y = torch.squeeze(Xtr[:5]), torch.squeeze(Ytr[:5])\n",
    "\n",
    "# logits = gpt(X)\n",
    "# print(logits[:,-1,:])\n",
    "# # out = logits[:,-1,:].view(-1, logits.size(-1))\n",
    "# print(out)\n",
    "# print(Y)\n",
    "# # pluck \n",
    "# dev_loss = F.cross_entropy(logits[:,-1,:], Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e8547",
   "metadata": {},
   "source": [
    "# Now let's make it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c982ac67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d10dbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to convert chars to int and inverse\n",
    "\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "\n",
    "# . is both \"before start\" in X, and \"im done\" for Y\n",
    "stoi['.'] = 0\n",
    "itos = {s:i for i,s in stoi.items()}\n",
    "\n",
    "num_char = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa59ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, device):\n",
    "    x, y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        for i, c in enumerate(word + '.'):\n",
    "            mini_x = []\n",
    "            for w in reversed(range(1, window+1)):\n",
    "                if i - w >= 0:\n",
    "                    mini_x.append(stoi[word[i-w]])\n",
    "                else:\n",
    "                    mini_x.append(stoi['.'])\n",
    "\n",
    "            x.append(mini_x)\n",
    "            y.append(stoi[c])\n",
    "            \n",
    "    return torch.tensor(x, device=device), torch.tensor(y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf26a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n1], device=cpu)\n",
    "Xdev, Ydev = build_dataset(names[n1:n2], device=cpu)\n",
    "Xte, Yte = build_dataset(names[n2:], device=cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933fc08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] --> c\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'c'] --> o\n"
     ]
    }
   ],
   "source": [
    "for i in range(2): \n",
    "    print(\"{} --> {}\".format([itos[c.item()] for c in Xtr[i]], itos[Ytr[i].item()]))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc2ae47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: dt: 3.90 dev_loss: 3.5128 loss: 3.6062\n",
      "   2000/ 200000: dt: 298.98 dev_loss: 2.5893 loss: 2.6829\n",
      "   4000/ 200000: dt: 586.79 dev_loss: 2.6213 loss: 2.3875\n",
      "   6000/ 200000: dt: 869.60 dev_loss: 2.9032 loss: 2.6299\n",
      "   8000/ 200000: dt: 1150.57 dev_loss: 2.4862 loss: 2.3863\n",
      "  10000/ 200000: dt: 1431.09 dev_loss: 2.6333 loss: 2.3692\n",
      "  12000/ 200000: dt: 1711.92 dev_loss: 2.3036 loss: 2.3886\n",
      "  14000/ 200000: dt: 1970.06 dev_loss: 2.1615 loss: 2.2835\n",
      "  16000/ 200000: dt: 2225.28 dev_loss: 2.4631 loss: 2.3602\n",
      "  18000/ 200000: dt: 2477.81 dev_loss: 2.2285 loss: 2.2893\n",
      "  20000/ 200000: dt: 2731.91 dev_loss: 2.3686 loss: 2.6120\n",
      "  22000/ 200000: dt: 2985.90 dev_loss: 2.6159 loss: 2.6038\n",
      "  24000/ 200000: dt: 3239.59 dev_loss: 2.3325 loss: 2.4089\n",
      "  26000/ 200000: dt: 3493.51 dev_loss: 2.6571 loss: 2.4474\n",
      "  28000/ 200000: dt: 3747.53 dev_loss: 2.1306 loss: 2.3210\n",
      "  30000/ 200000: dt: 4001.67 dev_loss: 2.1593 loss: 2.2871\n",
      "  32000/ 200000: dt: 4255.47 dev_loss: 2.4880 loss: 2.3838\n",
      "  34000/ 200000: dt: 4508.88 dev_loss: 2.5757 loss: 2.3269\n",
      "  36000/ 200000: dt: 4761.72 dev_loss: 2.4502 loss: 2.1450\n",
      "  38000/ 200000: dt: 5015.62 dev_loss: 2.2055 loss: 2.3469\n",
      "  40000/ 200000: dt: 5269.51 dev_loss: 2.5680 loss: 2.3148\n",
      "  42000/ 200000: dt: 5523.51 dev_loss: 2.5758 loss: 2.4839\n",
      "  44000/ 200000: dt: 5778.24 dev_loss: 2.1079 loss: 2.1508\n",
      "  46000/ 200000: dt: 6032.51 dev_loss: 2.4423 loss: 2.5853\n",
      "  48000/ 200000: dt: 6286.29 dev_loss: 2.4207 loss: 2.3430\n",
      "  50000/ 200000: dt: 6539.95 dev_loss: 2.2787 loss: 2.3861\n",
      "  52000/ 200000: dt: 6793.53 dev_loss: 2.2391 loss: 2.5026\n",
      "  54000/ 200000: dt: 7047.31 dev_loss: 2.2018 loss: 2.2206\n",
      "  56000/ 200000: dt: 7301.08 dev_loss: 2.2143 loss: 2.3300\n",
      "  58000/ 200000: dt: 7554.72 dev_loss: 2.4742 loss: 2.2171\n",
      "  60000/ 200000: dt: 7808.48 dev_loss: 2.3063 loss: 2.2662\n",
      "  62000/ 200000: dt: 8062.28 dev_loss: 2.3623 loss: 2.4302\n",
      "  64000/ 200000: dt: 8315.95 dev_loss: 2.3892 loss: 2.3347\n",
      "  66000/ 200000: dt: 8571.00 dev_loss: 2.1833 loss: 2.1035\n",
      "  68000/ 200000: dt: 8825.87 dev_loss: 2.2204 loss: 2.2984\n",
      "  70000/ 200000: dt: 9079.73 dev_loss: 2.2070 loss: 2.5479\n",
      "  72000/ 200000: dt: 9334.63 dev_loss: 2.3907 loss: 2.2646\n",
      "  74000/ 200000: dt: 9589.46 dev_loss: 2.3613 loss: 2.5356\n",
      "  76000/ 200000: dt: 9843.52 dev_loss: 2.2271 loss: 2.3206\n",
      "  78000/ 200000: dt: 10098.04 dev_loss: 2.2757 loss: 2.2188\n",
      "  80000/ 200000: dt: 10351.96 dev_loss: 2.4508 loss: 2.3241\n",
      "  82000/ 200000: dt: 10606.50 dev_loss: 2.2989 loss: 2.2812\n",
      "  84000/ 200000: dt: 10861.80 dev_loss: 1.9795 loss: 2.1652\n",
      "  86000/ 200000: dt: 11115.59 dev_loss: 2.5994 loss: 2.1912\n",
      "  88000/ 200000: dt: 11369.83 dev_loss: 2.1606 loss: 2.3030\n",
      "  90000/ 200000: dt: 11624.61 dev_loss: 2.1499 loss: 2.3878\n",
      "  92000/ 200000: dt: 11877.99 dev_loss: 2.4075 loss: 2.0146\n",
      "  94000/ 200000: dt: 12131.46 dev_loss: 2.3807 loss: 2.5002\n",
      "  96000/ 200000: dt: 12385.70 dev_loss: 2.1747 loss: 2.2297\n",
      "  98000/ 200000: dt: 12639.69 dev_loss: 2.3131 loss: 2.3020\n",
      " 100000/ 200000: dt: 12893.66 dev_loss: 2.3870 loss: 2.1347\n",
      " 102000/ 200000: dt: 13148.04 dev_loss: 2.2323 loss: 2.1114\n",
      " 104000/ 200000: dt: 13401.49 dev_loss: 2.2697 loss: 2.5519\n",
      " 106000/ 200000: dt: 13655.33 dev_loss: 2.0516 loss: 2.1482\n",
      " 108000/ 200000: dt: 13910.12 dev_loss: 2.1702 loss: 2.4167\n",
      " 110000/ 200000: dt: 14163.88 dev_loss: 2.3516 loss: 2.2031\n",
      " 112000/ 200000: dt: 14417.28 dev_loss: 2.0452 loss: 2.2967\n",
      " 114000/ 200000: dt: 14671.12 dev_loss: 2.2389 loss: 2.5071\n",
      " 116000/ 200000: dt: 14924.76 dev_loss: 2.3229 loss: 2.4974\n",
      " 118000/ 200000: dt: 15178.58 dev_loss: 2.0541 loss: 2.3628\n",
      " 120000/ 200000: dt: 15432.87 dev_loss: 2.3319 loss: 2.1181\n",
      " 122000/ 200000: dt: 15686.51 dev_loss: 2.2696 loss: 2.4847\n",
      " 124000/ 200000: dt: 15940.14 dev_loss: 2.1156 loss: 2.2119\n",
      " 126000/ 200000: dt: 16193.22 dev_loss: 2.3094 loss: 2.0904\n",
      " 128000/ 200000: dt: 16447.08 dev_loss: 2.3164 loss: 2.1673\n",
      " 130000/ 200000: dt: 16702.15 dev_loss: 2.0942 loss: 2.6011\n",
      " 132000/ 200000: dt: 16956.53 dev_loss: 2.1914 loss: 2.3037\n",
      " 134000/ 200000: dt: 17211.56 dev_loss: 2.3639 loss: 2.3149\n",
      " 136000/ 200000: dt: 17466.16 dev_loss: 2.2928 loss: 2.3273\n",
      " 138000/ 200000: dt: 17720.92 dev_loss: 2.3154 loss: 2.4929\n",
      " 140000/ 200000: dt: 17975.29 dev_loss: 1.9914 loss: 2.3559\n",
      " 142000/ 200000: dt: 18230.43 dev_loss: 2.4196 loss: 2.2364\n",
      " 144000/ 200000: dt: 18483.85 dev_loss: 2.3034 loss: 2.5570\n",
      " 146000/ 200000: dt: 18738.10 dev_loss: 2.2682 loss: 2.2793\n",
      " 148000/ 200000: dt: 18992.12 dev_loss: 2.2944 loss: 2.2690\n",
      " 150000/ 200000: dt: 19246.36 dev_loss: 2.2940 loss: 2.4467\n",
      " 152000/ 200000: dt: 19499.73 dev_loss: 2.1876 loss: 2.0977\n",
      " 154000/ 200000: dt: 19754.56 dev_loss: 2.4061 loss: 2.1038\n",
      " 156000/ 200000: dt: 20009.04 dev_loss: 2.2410 loss: 2.2882\n",
      " 158000/ 200000: dt: 20264.72 dev_loss: 2.2807 loss: 2.3345\n",
      " 160000/ 200000: dt: 20519.65 dev_loss: 2.1453 loss: 2.5052\n",
      " 162000/ 200000: dt: 20773.87 dev_loss: 2.0213 loss: 2.4364\n",
      " 164000/ 200000: dt: 21027.92 dev_loss: 2.3952 loss: 2.4258\n",
      " 166000/ 200000: dt: 21282.89 dev_loss: 2.1899 loss: 2.2180\n",
      " 168000/ 200000: dt: 21553.49 dev_loss: 2.2617 loss: 2.3286\n",
      " 170000/ 200000: dt: 21833.77 dev_loss: 2.3405 loss: 2.3024\n",
      " 172000/ 200000: dt: 22113.58 dev_loss: 2.3422 loss: 2.4259\n",
      " 174000/ 200000: dt: 22390.53 dev_loss: 2.3873 loss: 2.4237\n",
      " 176000/ 200000: dt: 22666.55 dev_loss: 2.3914 loss: 2.0982\n",
      " 178000/ 200000: dt: 22960.35 dev_loss: 2.1415 loss: 2.2938\n",
      " 180000/ 200000: dt: 23254.71 dev_loss: 2.5830 loss: 2.4286\n",
      " 182000/ 200000: dt: 23528.72 dev_loss: 2.0701 loss: 2.5691\n",
      " 184000/ 200000: dt: 23782.49 dev_loss: 2.5727 loss: 2.4963\n",
      " 186000/ 200000: dt: 24033.57 dev_loss: 2.5749 loss: 2.2976\n",
      " 188000/ 200000: dt: 24306.39 dev_loss: 2.1571 loss: 2.3206\n",
      " 190000/ 200000: dt: 24567.51 dev_loss: 2.2015 loss: 2.3162\n",
      " 192000/ 200000: dt: 24820.36 dev_loss: 1.9831 loss: 2.2629\n",
      " 194000/ 200000: dt: 25071.98 dev_loss: 2.2841 loss: 1.8735\n",
      " 196000/ 200000: dt: 25327.62 dev_loss: 1.9891 loss: 2.2682\n",
      " 198000/ 200000: dt: 25579.86 dev_loss: 2.1701 loss: 2.3821\n",
      " 200000/ 200000: dt: 25831.67 dev_loss: 2.4255 loss: 2.3524\n",
      "total training time: 25831.666651875003\n"
     ]
    }
   ],
   "source": [
    "network = GPT(d_model, heads, blocks)\n",
    "network.to(m1)\n",
    "network.train(mode=True)\n",
    "\n",
    "opt = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "steps = []\n",
    "losses = []\n",
    "dev_steps = []\n",
    "dev_losses = []\n",
    "batch_size = 64\n",
    "max_steps = 200000\n",
    "rec_freq = 2000\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for i in range(max_steps+1):\n",
    "    # sample from training set\n",
    "    sample_idx = torch.randint(len(Ytr), size=(batch_size,1))\n",
    "    X, Y = torch.squeeze(Xtr[sample_idx].to(m1)), torch.squeeze(Ytr[sample_idx].to(m1))\n",
    "    \n",
    "    # forward\n",
    "    logits = network(X)\n",
    "    loss = F.cross_entropy(logits[:,-1,:], Y)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    ## Record data\n",
    "    steps.append(i)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if i % rec_freq == 0: # print every once in a while\n",
    "        dev_loss = 0\n",
    "        with torch.no_grad():\n",
    "            dev_idx = torch.randint(len(Ydev), size=(batch_size,1))\n",
    "            X_check, Y_check = torch.squeeze(Xdev[dev_idx].to(m1)), torch.squeeze(Ydev[dev_idx].to(m1))\n",
    "\n",
    "            dev_loss = F.cross_entropy(network(X_check)[:,-1,:], Y_check)\n",
    "            \n",
    "        current_time = time.perf_counter()\n",
    "        dt = current_time - start_time\n",
    "        print(f'{i:7d}/{max_steps:7d}: dt: {dt:.2f} dev_loss: {dev_loss.item():.4f} loss: {loss.item():.4f}')\n",
    "        \n",
    "        dev_losses.append(dev_loss.item())\n",
    "        dev_steps.append(i)\n",
    "\n",
    "\n",
    "current_time = time.perf_counter()\n",
    "dt = current_time - start_time\n",
    "print(\"total training time: {}\".format(dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62e07b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6a8679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NameDataLoader():\n",
    "    def __init__(self, words):\n",
    "        self.X, self.Y = self._build_dataset(words)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.Y)\n",
    "        \n",
    "    def _build_dataset(self, words):\n",
    "        x, y = [], []\n",
    "\n",
    "        for word in words:\n",
    "            for i, c in enumerate(word + '.'):\n",
    "                mini_x = []\n",
    "                for w in reversed(range(1, window+1)):\n",
    "                    if i - w >= 0:\n",
    "                        mini_x.append(stoi[word[i-w]])\n",
    "                    else:\n",
    "                        mini_x.append(stoi['.'])\n",
    "\n",
    "                x.append(mini_x)\n",
    "                y.append(stoi[c])\n",
    "                \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "\n",
    "class NameData():\n",
    "    def __init__(self, name_txt_path):\n",
    "\n",
    "        self.names = open(name_txt_path, 'r').read().splitlines()\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.names)\n",
    "        n1 = int(0.8*len(self.names))\n",
    "        n2 = int(0.9*len(self.names))\n",
    "\n",
    "        self.train = NameDataLoader(self.names[:n1])\n",
    "        self.dev = NameDataLoader(self.names[n1:n2])\n",
    "        self.test = NameDataLoader(self.names[n2:])\n",
    "\n",
    "    def train_data_loader(self):\n",
    "        return self.train\n",
    "\n",
    "    def test_data_loader(self):\n",
    "        return self.test\n",
    "\n",
    "    def val_data_loader(self):\n",
    "        return self.dev\n",
    "\n",
    "\n",
    "data = NameData('compiled_names.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSurnames(LightningModule):\n",
    "    def __init__(self, data, d_model, heads, blocks, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # save hyper params\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.blocks = blocks\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if data is None:\n",
    "            data = NameData('compiled_names.txt')\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        self.model = GPT(\n",
    "            self.d_model, \n",
    "            self.heads, \n",
    "            self.blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits[:,-1,:], y)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        self.log('tr_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits[:,-1,:], y)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits[:,-1,:], y)\n",
    "\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data.train_data_loader(), batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data.test_data_loader(), batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data.val_data_loader(), batch_size=self.batch_size)\n",
    "\n",
    "run = wandb.init(project=\"surnamerator\", reinit=True)\n",
    "wandb_logger = WandbLogger()\n",
    "data, = None\n",
    "# chars in dictionary\n",
    "vocab = 30\n",
    "\n",
    "# d_model is the same as embedding same for simplicity.\n",
    "# embs = 12\n",
    "d_model = 12\n",
    "\n",
    "# number of chars to see in one window\n",
    "window = 16\n",
    "\n",
    "# This will increase one day\n",
    "batch_size = 64\n",
    "\n",
    "heads = 3\n",
    "\n",
    "blocks = 6\n",
    "lit_surname = LitSurnames()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "18e189b7a455dca94c157be0e0713a038d2605132cba86745af50b7dbf438d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
