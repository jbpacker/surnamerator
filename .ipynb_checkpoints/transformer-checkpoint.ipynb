{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e49678a4-3895-486c-a878-18b53d2f9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b1a67-473d-4496-bf2e-fa96a47c2e57",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68c93796-5f49-43c7-a272-99bf23dc04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch of 5, vocab of 30\n",
    "vocab = 30\n",
    "embs = 12\n",
    "batch_size = 5\n",
    "window = 16\n",
    "hidden_nodes = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06144259-ded5-46fe-ae47-d4262846e92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 16]), torch.Size([5, 16, 12]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = torch.randint(vocab, (batch_size, window))\n",
    "emb_prompts = nn.Embedding(vocab, embs)(prompts)\n",
    "prompts.shape, emb_prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7723746e-387a-422a-a209-d1f5f93b922d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 12])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Thank you! https://jalammar.github.io/illustrated-transformer/\n",
    "# for evens:  sin(pos/10000**(2i/embs))\n",
    "# for odds:   cos(pos/10000**(2i/embs))\n",
    "pos_idxs = torch.arange(0, window).view((-1, 1))\n",
    "emb_idxs = torch.arange(0, embs).view((1, -1))\n",
    "\n",
    "angles = pos_idxs / (10000**(2*emb_idxs / embs))\n",
    "\n",
    "pos_enc = torch.zeros_like(angles)\n",
    "pos_enc[:, 0::2] += torch.sin(angles[:, 0::2])\n",
    "pos_enc[:, 1::2] += torch.cos(angles[:, 1::2])\n",
    "\n",
    "## Plot the positional embeddings\n",
    "# plt.pcolormesh(pos_enc, cmap='viridis')\n",
    "# plt.xlabel('Embedding Dimensions')\n",
    "# plt.xlim((0, embs))\n",
    "# plt.ylim((window,0))\n",
    "# plt.ylabel('Char Position')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "pos_enc.shape\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, window):\n",
    "        super(PositionalEncoding, self)._init__()\n",
    "        \n",
    "        pos_idxs = torch.arange(0, window).view(-1, 1)\n",
    "        emb_idxs = torch.arange(0, d_model).view(1, -1)\n",
    "        \n",
    "        \n",
    "        angles = pos_idxs / (10000**(2*emb_idxs / embs))\n",
    "\n",
    "        pos_enc = torch.zeros_like(angles)\n",
    "        pos_enc[:, 0::2] += torch.sin(angles[:, 0::2])\n",
    "        pos_enc[:, 1::2] += torch.cos(angles[:, 1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "390584d8-3905-4eb3-851e-b0e3561b902e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 16, 12]), torch.Size([5, 16, 12]), torch.Size([5, 16, 24]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc = pos_enc.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "embedded_prompt = torch.cat((emb_prompts, pos_enc2), dim=2)\n",
    "emb_prompts.shape, pos_enc.shape, embedded_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88bed23f-ed4d-4e1b-90fc-61f31dcd67bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: torch.Size([5, 16, 24])\n",
      "Q: torch.Size([5, 16, 48])\n",
      "V: torch.Size([5, 16, 48])\n",
      "K: torch.Size([5, 16, 48])\n",
      "K.transpose(-2, -1): torch.Size([5, 48, 16])\n",
      "score: torch.Size([5, 16, 16])\n",
      "gain: torch.Size([5, 16, 16])\n",
      "softmax: torch.Size([5, 16, 16])\n",
      "out: torch.Size([5, 16, 48])\n"
     ]
    }
   ],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "\n",
    "print(\"prompt: \" + str(embedded_prompt.shape))\n",
    "\n",
    "Q = nn.Linear(24, hidden_nodes)(embedded_prompt)\n",
    "V = nn.Linear(24, hidden_nodes)(embedded_prompt)\n",
    "K = nn.Linear(24, hidden_nodes)(embedded_prompt)\n",
    "\n",
    "d_k = emb_prompts.shape[2]\n",
    "\n",
    "print(\"Q: \" + str(Q.shape))\n",
    "print(\"V: \" + str(V.shape))\n",
    "print(\"K: \" + str(K.shape))\n",
    "print(\"K.transpose(-2, -1): \" + str(K.transpose(-2, -1).shape))\n",
    "\n",
    "score = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "print(\"score: \" + str(score.shape))\n",
    "\n",
    "gain = score / math.sqrt(K.shape[-1])\n",
    "\n",
    "print(\"gain: \" + str(gain.shape))\n",
    "\n",
    "softmax = nn.Softmax(dim=1)(gain) \n",
    "\n",
    "print(\"softmax: \" + str(softmax.shape))\n",
    "\n",
    "out = softmax @ K\n",
    "\n",
    "print(\"out: \" + str(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763b3b2-8f6a-4412-8802-3f529b847bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.hidden = hidden_nodes\n",
    "\n",
    "        Wq = nn.Linear(self.input, self.hidden)\n",
    "        Wv = nn.Linear(self.input, self.hidden)\n",
    "        Wk = nn.Linear(self.input, self.hidden)\n",
    "        \n",
    "    def forward(self, q, v, k):\n",
    "        \n",
    "        \n",
    "        q = Wq(x)\n",
    "        v = Wv(x)\n",
    "        k = Wk(x)\n",
    "\n",
    "        d_k = emb_prompts.shape[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
