{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT for surname creation\n",
    "\n",
    "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class GptAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    For this attention module k = v = q are all the same.\n",
    "    It's for encoder/decoder only transfomers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(GptAttention, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config[\"d_model\"] % self.config[\"heads\"] == 0\n",
    "        self.heads = self.config[\"heads\"]\n",
    "\n",
    "        self.w_attn = nn.Linear(self.config[\"d_model\"], 3*self.config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"d_model\"])\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config[\"attn_pdrop\"])\n",
    "        self.resid_dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\", \n",
    "            torch.tril(\n",
    "                torch.ones(\n",
    "                    self.config[\"window\"], \n",
    "                    self.config[\"window\"])\n",
    "                ).view(1, 1, self.config[\"window\"], self.config[\"window\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, window, embs = x.shape\n",
    "\n",
    "        q, v, k = self.w_attn(x).split(self.config[\"d_model\"], dim=2)\n",
    "\n",
    "        # (B, heads, window, embs)\n",
    "        q = q.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        k = k.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        v = v.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # Self-attend: (B, heads, window, embs) x (B, heads, embs, window) -> (B, heads, window, window)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        mask = scores.masked_fill(self.bias[:,:,:window,:window] == 0, float('-inf'))\n",
    "        probs = F.softmax(mask, dim=-1)\n",
    "        attn = self.attn_dropout(probs)\n",
    "        attn = probs @ v\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, window, embs)\n",
    "\n",
    "        return self.resid_dropout(self.head(attn))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(config[\"d_model\"], 4*config[\"d_model\"])\n",
    "        self.l2 = nn.Linear(4*config[\"d_model\"], config[\"d_model\"])\n",
    "        self.dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = NewGELU()(self.l1(x))\n",
    "        return self.dropout(self.l2(x))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.attn = GptAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm2 = nn.LayerNorm(config[\"d_model\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "# gpt_attn = GptAttention(heads, d_model)\n",
    "# out = gpt_attn(enc_prompt)\n",
    "# print(out.shape)\n",
    "\n",
    "# b = Block(d_model, 3)\n",
    "# out = b(emb_prompts)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.vocab_emb = nn.Embedding(self.config[\"vocab\"], self.config[\"d_model\"])\n",
    "        self.pos_emb = nn.Embedding(self.config[\"window\"], self.config[\"d_model\"])\n",
    "        self.emb_dropout = nn.Dropout(config[\"embd_pdrop\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(self.config) for _ in range(self.config[\"blocks\"])])\n",
    "        self.head_layer_norm = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"vocab\"])\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config[\"n_layer\"]))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        vocab_emb = self.vocab_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(0, x.shape[1], dtype=torch.long, device=x.device))\n",
    "\n",
    "        x = self.emb_dropout(vocab_emb + pos_emb)\n",
    "\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "\n",
    "        x = self.head_layer_norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_opt(self):\n",
    "        p_decay = set()\n",
    "        p_no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    p_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = p_decay & p_no_decay\n",
    "        union_params = p_decay | p_no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_decay))], \"weight_decay\": self.config[\"weight_decay\"]},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, \n",
    "            lr=self.config[\"lr\"], \n",
    "            betas=(self.config[\"b1\"], self.config[\"b2\"])\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader for names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataLoader():\n",
    "    \"\"\"\n",
    "    Creates a dataset based on a list of names. All letters will be shifted by one when comparing input and output.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Basic usage\n",
    "    - prepend_syllables=False\n",
    "    - prepend_num_last_chars =0\n",
    "    Name = 'jef'\n",
    "    x[0] --> y[0] = .......... --> .........j\n",
    "    x[1] --> y[1] = .........j --> ........je\n",
    "    x[2] --> y[2] = ........je --> .......jef\n",
    "    x[3] --> y[3] = .......jef --> ......jef.\n",
    "\n",
    "    syllables\n",
    "    NOTE: 'jef' has 1 syllable, and itos[1] = ' '\n",
    "    - prepend_syllables=True\n",
    "    - prepend_num_last_chars=0\n",
    "    Name = 'jef'\n",
    "    x[0] --> y[0] = ........ . --> ....... .j\n",
    "    x[1] --> y[1] = ....... .j --> ...... .je\n",
    "    x[2] --> y[2] = ...... .je --> ..... .jef\n",
    "    x[3] --> y[3] = ..... .jef --> .... .jef.\n",
    "\n",
    "    prepend_num_last_chars\n",
    "    - prepend_syllables=True\n",
    "    - prepend_num_last_chars=2\n",
    "    Name = 'jef'\n",
    "    x[0] --> y[0] = .....ef.1. --> ....ef.1.j\n",
    "    x[1] --> y[1] = ....ef.1.j --> ...ef.1.je\n",
    "    x[2] --> y[2] = ...ef.1.je --> ..ef.1.jef\n",
    "    x[3] --> y[3] = ..ef.1.jef --> .ef.1.jef.\n",
    "\n",
    "    NOTE: Data Loader can be extended with other dataloaders for extreme data augmentation!\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, words, window, stoi, itos, prepend_syllables=False, prepend_num_last_chars=0):\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.window = window\n",
    "        self.prepend_syllables = prepend_syllables\n",
    "        self.prepend_num_last_chars = prepend_num_last_chars\n",
    "\n",
    "        self.X, self.Y = self._build_dataset(words)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.Y)\n",
    "        \n",
    "    def _count_syllables(self, word):\n",
    "        word = word.lower()\n",
    "        count = 0\n",
    "        vowels = \"aeiouy\"\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "\n",
    "        # NOTE: this will break if there are 3 vowels in a row\n",
    "        for index in range(1, len(word)):\n",
    "            if word[index] in vowels and word[index - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word.endswith(\"e\"):\n",
    "            count -= 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "    def _build_dataset(self, words):\n",
    "        x, y = [], []\n",
    "\n",
    "        for name in words:\n",
    "            ctx = [0] * self.window\n",
    "\n",
    "            if (self.prepend_num_last_chars > 0):\n",
    "                \"\"\"\n",
    "                Add final chars to the end of the buffer\n",
    "                \"\"\"\n",
    "                ori_len = len(name)\n",
    "\n",
    "                ctx.pop(0)\n",
    "                ctx.append(self.stoi['.'])\n",
    "\n",
    "                for i in reversed(range(1, min(self.prepend_num_last_chars+1, len(name)+1))):\n",
    "                    ctx.pop(0)\n",
    "                    ctx.append(self.stoi[name[-i]])\n",
    "\n",
    "                ctx.pop(0)\n",
    "                ctx.append(self.stoi['.'])\n",
    "\n",
    "            if self.prepend_syllables:\n",
    "                num_syllables = self._count_syllables(name)\n",
    "                ctx.pop(0)\n",
    "                ctx.pop(0)\n",
    "                ctx.append(num_syllables)\n",
    "                ctx.append(self.stoi['.'])\n",
    "\n",
    "                # name = self.itos[num_syllables] + '.' + name\n",
    "\n",
    "            for c in (name + '.'):\n",
    "                x.append(copy.deepcopy(ctx))\n",
    "                ctx.pop(0)\n",
    "                ctx.append(self.stoi[c])\n",
    "                y.append(copy.deepcopy(ctx))\n",
    "                \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "    def cat(self, new_data_loader):\n",
    "        self.X = torch.cat((self.X, new_data_loader.X))\n",
    "        self.Y = torch.cat((self.Y, new_data_loader.Y))\n",
    "\n",
    "    def debug_print(self, i_start, i_end):\n",
    "        for i in range(i_start, i_end):\n",
    "            print(\"\".join([self.itos[c.item()] for c in self.X[i]]) + \" --> \" + \"\".join([self.itos[c.item()] for c in self.Y[i]]))\n",
    "\n",
    "\n",
    "class NameData():\n",
    "    def __init__(self, name_txt_path, window, num_final_chars):\n",
    "\n",
    "        self.window = window\n",
    "        self.names = open(name_txt_path, 'r').read().splitlines()\n",
    "\n",
    "        self.stoi, self.itos = self._make_stoi_and_itos(self.names)\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.names)\n",
    "        n1 = int(0.8*len(self.names))\n",
    "        n2 = int(0.9*len(self.names))\n",
    "\n",
    "        self.train, self.dev, self.test = self.make_data_loaders(n1, n2, prepend_syllables=False, prepend_num_last_chars=0)\n",
    "        for i in range(1, num_final_chars + 1):\n",
    "            self.cat_data_loaders(n1, n2, prepend_syllables=False, prepend_num_last_chars=i)\n",
    "\n",
    "    def cat_data_loaders(self, n1, n2, prepend_syllables, prepend_num_last_chars):\n",
    "        new_train, new_dev, new_test = self.make_data_loaders(n1, n2, prepend_syllables, prepend_num_last_chars)\n",
    "        self.train.cat(new_train)\n",
    "        self.dev.cat(new_dev)\n",
    "        self.test.cat(new_test)\n",
    "\n",
    "    def make_data_loaders(self, n1, n2, prepend_syllables=False, prepend_num_last_chars=0):\n",
    "        train = NameDataLoader(self.names[:n1], self.window, self.stoi, self.itos, prepend_syllables, prepend_num_last_chars)\n",
    "        dev = NameDataLoader(self.names[n1:n2], self.window, self.stoi, self.itos, prepend_syllables, prepend_num_last_chars)\n",
    "        test = NameDataLoader(self.names[n2:], self.window, self.stoi, self.itos, prepend_syllables, prepend_num_last_chars)\n",
    "        return train, dev, test\n",
    "\n",
    "    def _make_stoi_and_itos(self, names):\n",
    "        ## functions to convert chars to int and inverse\n",
    "        chars = sorted(list(set(''.join(names))))\n",
    "        stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "\n",
    "        # . is both \"before start\" in X, and \"im done\" for Y\n",
    "        stoi['.'] = 0\n",
    "        itos = {s:i for i,s in stoi.items()}\n",
    "\n",
    "        return stoi, itos\n",
    "\n",
    "    def stoi(self, char):\n",
    "        return self.stoi[char]\n",
    "    \n",
    "    def itos(self, i):\n",
    "        return self.itos[i]\n",
    "\n",
    "    def vocab(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def train_data_loader(self):\n",
    "        return self.train\n",
    "\n",
    "    def test_data_loader(self):\n",
    "        return self.test\n",
    "\n",
    "    def val_data_loader(self):\n",
    "        return self.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSurnames(LightningModule):\n",
    "    def __init__(self, config, data_file):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.data = NameData(\n",
    "            data_file, \n",
    "            self.config[\"window\"], \n",
    "            self.config[\"num_final_chars_in_dataset\"]\n",
    "        )\n",
    "        self.config[\"vocab\"] = self.data.vocab()\n",
    "\n",
    "        self.model = GPT(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        return F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch)\n",
    "        self.log('tr_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.model.configure_opt()\n",
    "\n",
    "    def generate_name(self, first_chars = \"\", final_chars = \"\"):\n",
    "        self.eval()\n",
    "\n",
    "        name = \"\"\n",
    "        ctx = [0] * self.config['window']\n",
    "\n",
    "        # Put final chars in context\n",
    "        if len(final_chars) > self.config[\"num_final_chars_in_dataset\"]:\n",
    "            final_chars = final_chars[-self.config[\"num_final_chars_in_dataset\"]:]\n",
    "            print(\"Only accepts up to \" + str(self.config[\"num_final_chars_in_dataset\"]) + \" final chars. Using: \" + final_chars)\n",
    "                \n",
    "        for c in final_chars:\n",
    "            ctx = ctx[1:] + [self.data.stoi[c]]\n",
    "        ctx = ctx[1:] + [self.data.stoi['.']]\n",
    "\n",
    "        # put first chars both in name and context\n",
    "        for c in first_chars:\n",
    "            name += c\n",
    "            ctx = ctx[1:] + [self.data.stoi[c]]\n",
    "\n",
    "        # Run inference to finish off the name!\n",
    "        for _ in range(80):\n",
    "            logits = self(torch.tensor(ctx).view(1, -1))\n",
    "            probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            ctx = ctx[1:] + [ix]\n",
    "            name += self.data.itos[ix]\n",
    "            if ix == 0:\n",
    "                break\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        return name\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data.train_data_loader(), \n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            # num_workers = config[\"num_workers\"]\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data.test_data_loader(), \n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data.val_data_loader(), \n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "    \"model_type\": 'gpt',\n",
    "\n",
    "    # Window must remain the same for the losses to make sense!!\n",
    "    \"window\": 32,\n",
    "\n",
    "    ## Tiny network, for smoke testing\n",
    "    # \"blocks\": 3,\n",
    "    # \"heads\": 1,\n",
    "    # \"d_model\":  4,\n",
    "\n",
    "    ## Pico network\n",
    "    \"blocks\": 1,\n",
    "    \"heads\": 3,\n",
    "    \"d_model\": 48,\n",
    "\n",
    "    ## Nano network\n",
    "    # \"blocks\": 3,\n",
    "    # \"heads\": 3,\n",
    "    # \"d_model\": 48,\n",
    "\n",
    "    ## Micro\n",
    "    # \"blocks\": 6,\n",
    "    # \"heads\": 4,\n",
    "    # \"d_model\": 128,\n",
    "\n",
    "    ## Mini\n",
    "    # \"blocks\": 6,\n",
    "    # \"heads\": 6,\n",
    "    # \"d_model\": 192,\n",
    "\n",
    "    ## gpt\n",
    "    # \"blocks\": 12,\n",
    "    # \"heads\": 12,\n",
    "    # \"d_model\":768,\n",
    "\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"lr\": 3e-4,\n",
    "    # \"lr\": 5e-4,\n",
    "    \"b1\": 0.9,\n",
    "    \"b2\": 0.95,\n",
    "\n",
    "    # these options must be filled in externally\n",
    "    \"vocab\": None,\n",
    "\n",
    "    # Dropout hyperparameters\n",
    "    \"embd_pdrop\": 0.1,\n",
    "    \"resid_pdrop\": 0.1,\n",
    "    \"attn_pdrop\": 0.1,\n",
    "\n",
    "    # Training parameters\n",
    "    \"batch_size\": 1024,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 500000,\n",
    "\n",
    "    # Dataset parameters\n",
    "    \"num_final_chars_in_dataset\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.03M\n"
     ]
    }
   ],
   "source": [
    "lit_surname = LitSurnames(config, 'surnames.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3825924, 32])\n",
      ".........................hl.gohl --> ........................hl.gohl.\n",
      ".............................eh. --> ............................eh.j\n",
      "............................eh.j --> ...........................eh.ja\n",
      "...........................eh.ja --> ..........................eh.jan\n",
      "..........................eh.jan --> .........................eh.jann\n",
      ".........................eh.jann --> ........................eh.janne\n",
      "........................eh.janne --> .......................eh.janneh\n",
      ".......................eh.janneh --> ......................eh.janneh.\n",
      ".............................rd. --> ............................rd.p\n",
      "............................rd.p --> ...........................rd.pa\n",
      "...........................rd.pa --> ..........................rd.pai\n",
      "..........................rd.pai --> .........................rd.pail\n",
      ".........................rd.pail --> ........................rd.paill\n",
      "........................rd.paill --> .......................rd.pailla\n",
      ".......................rd.pailla --> ......................rd.paillar\n",
      "......................rd.paillar --> .....................rd.paillard\n",
      ".....................rd.paillard --> ....................rd.paillard.\n",
      ".............................an. --> ............................an.b\n",
      "............................an.b --> ...........................an.be\n",
      "...........................an.be --> ..........................an.bel\n",
      "..........................an.bel --> .........................an.bels\n",
      ".........................an.bels --> ........................an.belsa\n",
      "........................an.belsa --> .......................an.belsan\n",
      ".......................an.belsan --> ......................an.belsan.\n"
     ]
    }
   ],
   "source": [
    "print(lit_surname.data.train.X.shape)\n",
    "lit_surname.data.train.debug_print(3825900, 3825924)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /Users/jef/stash/surnamerator exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 32.8 K\n",
      "-------------------------------\n",
      "32.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "32.8 K    Total params\n",
      "0.131     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03feab3961a34334a11976327fd1c619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3568651f0743baa00acc52e8f8b960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "use_wandb = False\n",
    "if use_wandb:\n",
    "    run = wandb.init(project=\"surnamerator\", reinit=True)\n",
    "    logger = WandbLogger()\n",
    "else:\n",
    "    import os\n",
    "    logger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=\"./\",\n",
    "    filename='gpt-surnames-{epoch:02d}')\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"mps\",\n",
    "    devices=1,\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor, checkpoint]\n",
    ")\n",
    "\n",
    "trainer.fit(lit_surname)\n",
    "\n",
    "if use_wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_pdrop: 0.1\n",
      "b1: 0.9\n",
      "b2: 0.95\n",
      "batch_size: 1024\n",
      "blocks: 6\n",
      "d_model: 128\n",
      "embd_pdrop: 0.1\n",
      "epochs: 500000\n",
      "heads: 4\n",
      "lr: 0.0003\n",
      "model_type: gpt\n",
      "num_workers: 4\n",
      "resid_pdrop: 0.1\n",
      "stoi:\n",
      "  ' ': 1\n",
      "  '''': 2\n",
      "  '-': 3\n",
      "  .: 0\n",
      "  a: 4\n",
      "  b: 5\n",
      "  c: 6\n",
      "  d: 7\n",
      "  e: 8\n",
      "  f: 9\n",
      "  g: 10\n",
      "  h: 11\n",
      "  i: 12\n",
      "  j: 13\n",
      "  k: 14\n",
      "  l: 15\n",
      "  m: 16\n",
      "  n: 17\n",
      "  o: 18\n",
      "  p: 19\n",
      "  q: 20\n",
      "  r: 21\n",
      "  s: 22\n",
      "  t: 23\n",
      "  u: 24\n",
      "  v: 25\n",
      "  w: 26\n",
      "  x: 27\n",
      "  y: 28\n",
      "  z: 29\n",
      "vocab: 30\n",
      "weight_decay: 0.1\n",
      "window: 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Save model and params\n",
    "import yaml\n",
    "\n",
    "config[\"stoi\"] = lit_surname.data.stoi\n",
    "\n",
    "with open('gpt_config.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "    \n",
    "print(open('gpt_config.yaml').read())\n",
    "\n",
    "# Need to save w/ cpu because spaces doesn't support device('mps')\n",
    "torch.save(lit_surname.model.to(torch.device(\"cpu\")).state_dict(), \"micro_gpt_weights.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what the model does at each step of inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................la.\n",
      ".............................la. --> ...............................b\n",
      "............................la.s --> ............................e.bt\n",
      "...........................la.st --> ...........................e.bca\n",
      "..........................la.stl --> ..........................e.bcaa\n",
      ".........................la.stla --> .........................e.bcaan\n",
      "........................la.stlac --> ........................e.bcaanh\n",
      ".......................la.stlaci --> .......................e.bcranhn\n",
      "......................la.stlacic --> ......................e.bcranhnh\n",
      ".....................la.stlacicz --> .....................e.bcranhnh.\n",
      "....................la.stlacicza --> ....................e.bcranhnhe.\n",
      "...................la.stlaciczak --> ...................e.bcranhnhan.\n",
      "..................la.stlaciczake --> ..................enb.ranhnhani.\n",
      "stlaciczake.\n"
     ]
    }
   ],
   "source": [
    "name = \"\"\n",
    "ctx = [0] * config['window']\n",
    "\n",
    "lit_surname.eval()\n",
    "\n",
    "itos = lit_surname.data.itos\n",
    "stoi = lit_surname.data.stoi\n",
    "\n",
    "final_chars = \"la\"\n",
    "if final_chars != \"\":\n",
    "    for c in final_chars:\n",
    "        ctx = ctx[1:] + [stoi[c]]\n",
    "\n",
    "    ctx = ctx[1:] + [stoi['.']]\n",
    "\n",
    "for c in \"\":\n",
    "    name += c\n",
    "    # print(lit_surname.data.stoi[c])\n",
    "    ctx = ctx[1:] + [stoi[c]]\n",
    "\n",
    "print(\"\".join([itos[c] for c in ctx]))\n",
    "\n",
    "# Run inference to finish off the name!\n",
    "for _ in range(80):\n",
    "    logits = lit_surname(torch.tensor(ctx).view(1, -1))\n",
    "    probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "\n",
    "    # samping vs expected\n",
    "    # ix = torch.max(probs, dim=1).indices.squeeze().item()\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    input = ctx\n",
    "    output = F.softmax(logits, dim=2)\n",
    "    output = torch.max(output, dim=2).indices.squeeze()\n",
    "\n",
    "    print(\"\".join([itos[c] for c in ctx]) + \" --> \" + \"\".join([itos[c.item()] for c in output]))\n",
    "\n",
    "    ctx = ctx[1:] + [ix]\n",
    "    name += lit_surname.data.itos[ix]\n",
    "    if ix == 0:\n",
    "        break\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### app for bulk name generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only accepts up to 3 final chars. Using: aa\n",
      "Only accepts up to 3 final chars. Using: aa\n",
      "Only accepts up to 3 final chars. Using: aa\n",
      "Only accepts up to 3 final chars. Using: aa\n",
      "Only accepts up to 3 final chars. Using: aa\n"
     ]
    }
   ],
   "source": [
    "def generate_name(first_chars = \"\", final_chars = \"\"):\n",
    "    lit_surname.eval()\n",
    "\n",
    "    name = \"\"\n",
    "    ctx = [0] * config['window']\n",
    "\n",
    "    # Put final chars in context\n",
    "    if len(final_chars) > config[\"num_final_chars_in_dataset\"]:\n",
    "        final_chars = final_chars[-config[\"num_final_chars_in_dataset\"]:]\n",
    "        print(\"Only accepts up to 3 final chars. Using: \" + final_chars)\n",
    "            \n",
    "    for c in final_chars:\n",
    "        ctx = ctx[1:] + [lit_surname.data.stoi[c]]\n",
    "    ctx = ctx[1:] + [lit_surname.data.stoi['.']]\n",
    "\n",
    "    # put first chars both in name and context\n",
    "    for c in first_chars:\n",
    "        name += c\n",
    "        ctx = ctx[1:] + [lit_surname.data.stoi[c]]\n",
    "\n",
    "    # Run inference to finish off the name!\n",
    "    for _ in range(80):\n",
    "        logits = lit_surname(torch.tensor(ctx).view(1, -1))\n",
    "        probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        ctx = ctx[1:] + [ix]\n",
    "        name += lit_surname.data.itos[ix]\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    lit_surname.train()\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def generate_names(name_start, name_end, number_of_names):\n",
    "    names = \"\"\n",
    "    for _ in range((int)(number_of_names)):\n",
    "    \n",
    "        # Initialize name with user input\n",
    "        names += generate_name(\n",
    "            name_start, name_end) + \"\\n\"\n",
    "\n",
    "    return names\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_names,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Start name with...\"),\n",
    "        gr.Textbox(placeholder=\"End name with...\"),\n",
    "        gr.Number(value=5),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    ")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
