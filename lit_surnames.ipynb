{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class GptAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    For this attention module k = v = q are all the same.\n",
    "    It's for encoder only transfomers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(GptAttention, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config[\"d_model\"] % self.config[\"heads\"] == 0\n",
    "        self.heads = self.config[\"heads\"]\n",
    "\n",
    "        self.w_attn = nn.Linear(self.config[\"d_model\"], 3*self.config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"d_model\"])\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config[\"attn_pdrop\"])\n",
    "        self.resid_dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\", \n",
    "            torch.tril(\n",
    "                torch.ones(\n",
    "                    self.config[\"window\"], \n",
    "                    self.config[\"window\"])\n",
    "                ).view(1, 1, self.config[\"window\"], self.config[\"window\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, window, embs = x.shape\n",
    "\n",
    "        q, v, k = self.w_attn(x).split(self.config[\"d_model\"], dim=2)\n",
    "\n",
    "        # (B, heads, window, embs)\n",
    "        q = q.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        k = k.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        v = v.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # Self-attend: (B, heads, window, embs) x (B, heads, embs, window) -> (B, heads, window, window)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        mask = scores.masked_fill(self.bias[:,:,:window,:window] == 0, float('-inf'))\n",
    "        probs = F.softmax(mask, dim=-1)\n",
    "        attn = self.attn_dropout(probs)\n",
    "        attn = probs @ v\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, window, embs)\n",
    "\n",
    "        return self.resid_dropout(self.head(attn))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(config[\"d_model\"], 4*config[\"d_model\"])\n",
    "        self.l2 = nn.Linear(4*config[\"d_model\"], config[\"d_model\"])\n",
    "        self.dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = NewGELU()(self.l1(x))\n",
    "        return self.dropout(self.l2(x))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.attn = GptAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm2 = nn.LayerNorm(config[\"d_model\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: these are re-ordered in andrej's code\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "# gpt_attn = GptAttention(heads, d_model)\n",
    "# out = gpt_attn(enc_prompt)\n",
    "# print(out.shape)\n",
    "\n",
    "# b = Block(d_model, 3)\n",
    "# out = b(emb_prompts)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.vocab_emb = nn.Embedding(self.config[\"vocab\"], self.config[\"d_model\"])\n",
    "        self.pos_emb = nn.Embedding(self.config[\"window\"], self.config[\"d_model\"])\n",
    "        self.emb_dropout = nn.Dropout(config[\"embd_pdrop\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(self.config) for _ in range(self.config[\"blocks\"])])\n",
    "        self.head_layer_norm = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"vocab\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        vocab_emb = self.vocab_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(0, x.shape[1], dtype=torch.long, device=x.device))\n",
    "\n",
    "        x = self.emb_dropout(vocab_emb + pos_emb)\n",
    "\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "\n",
    "        x = self.head_layer_norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_opt(self):\n",
    "        p_decay = set()\n",
    "        p_no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    p_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = p_decay & p_no_decay\n",
    "        union_params = p_decay | p_no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_decay))], \"weight_decay\": self.config[\"weight_decay\"]},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, \n",
    "            lr=self.config[\"lr\"], \n",
    "            betas=(self.config[\"b1\"], self.config[\"b2\"])\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader for names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NameDataLoader():\n",
    "    def __init__(self, words, window, stoi):\n",
    "        self.X, self.Y = self._build_dataset(words, window, stoi)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.Y)\n",
    "        \n",
    "    def _build_dataset(self, words, window, stoi):\n",
    "        x, y = [], []\n",
    "\n",
    "        for name in words:\n",
    "            ctx = [0] * window\n",
    "            for c in name:\n",
    "                x.append(copy.deepcopy(ctx))\n",
    "                ctx.pop(0)\n",
    "                ctx.append(stoi[c])\n",
    "                y.append(copy.deepcopy(ctx))\n",
    "                \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "    def debug_print(self, i_start, i_end):\n",
    "        for i in range(i_start, i_end):\n",
    "            print(\"\".join([itos[c.item()] for c in self.X[i]]) + \" --> \" + \"\".join([itos[c.item()] for c in self.Y[i]]))\n",
    "\n",
    "\n",
    "class NameData():\n",
    "    def __init__(self, name_txt_path, window):\n",
    "\n",
    "        self.window = window\n",
    "        self.names = open(name_txt_path, 'r').read().splitlines()\n",
    "\n",
    "        self.stoi, self.itos = self._make_stoi_and_itos(self.names)\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.names)\n",
    "        n1 = int(0.8*len(self.names))\n",
    "        n2 = int(0.9*len(self.names))\n",
    "\n",
    "        self.train = NameDataLoader(self.names[:n1], window, self.stoi)\n",
    "        self.dev = NameDataLoader(self.names[n1:n2], window, self.stoi)\n",
    "        self.test = NameDataLoader(self.names[n2:], window, self.stoi)\n",
    "\n",
    "    def _make_stoi_and_itos(self, names):\n",
    "        ## functions to convert chars to int and inverse\n",
    "        chars = sorted(list(set(''.join(names))))\n",
    "        stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "\n",
    "        # . is both \"before start\" in X, and \"im done\" for Y\n",
    "        stoi['.'] = 0\n",
    "        itos = {s:i for i,s in stoi.items()}\n",
    "\n",
    "        return stoi, itos\n",
    "\n",
    "    def stoi(self, char):\n",
    "        return self.stoi[char]\n",
    "    \n",
    "    def itos(self, i):\n",
    "        return self.itos[i]\n",
    "\n",
    "    def vocab(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def train_data_loader(self):\n",
    "        return self.train\n",
    "\n",
    "    def test_data_loader(self):\n",
    "        return self.test\n",
    "\n",
    "    def val_data_loader(self):\n",
    "        return self.dev\n",
    "\n",
    "\n",
    "# data = NameData('compiled_names.txt', 5)\n",
    "# data.train.debug_print(4, 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "    \"model_type\": 'gpt',\n",
    "\n",
    "    # Window must remain the same for the losses to make sense!!\n",
    "    \"window\": 32,\n",
    "\n",
    "    ## Tiny network, for smoke testing\n",
    "    # \"blocks\": 3,\n",
    "    # \"heads\": 1,\n",
    "    # \"d_model\":  4,\n",
    "\n",
    "    ## Pico network\n",
    "    # \"blocks\": 6,\n",
    "    # \"heads\": 4,\n",
    "    # \"d_model\": 8,\n",
    "\n",
    "    ## Nano network\n",
    "    # \"blocks\": 4,\n",
    "    # \"heads\": 4,\n",
    "    # \"d_model\": 64,\n",
    "\n",
    "    ## Micro\n",
    "    # \"blocks\": 6,\n",
    "    # \"heads\": 4,\n",
    "    # \"d_model\": 128,\n",
    "\n",
    "    ## Mini\n",
    "    \"blocks\": 6,\n",
    "    \"heads\": 6,\n",
    "    \"d_model\": 192,\n",
    "\n",
    "    ## gpt\n",
    "    # \"blocks\": 12,\n",
    "    # \"heads\": 12,\n",
    "    # \"d_model\":768,\n",
    "\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"lr\": 3e-4,\n",
    "    # \"lr\": 5e-4,\n",
    "    \"b1\": 0.9,\n",
    "    \"b2\": 0.95,\n",
    "\n",
    "    # these options must be filled in externally\n",
    "    \"vocab\": None,\n",
    "\n",
    "    # Dropout hyperparameters\n",
    "    \"embd_pdrop\": 0.1,\n",
    "    \"resid_pdrop\": 0.1,\n",
    "    \"attn_pdrop\": 0.1,\n",
    "\n",
    "    # Training parameters\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 4,\n",
    "    \"epochs\": 5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jef/stash/surnamerator/wandb/run-20221201_160843-4xc17g0o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jefsnacker/surnamerator/runs/4xc17g0o\" target=\"_blank\">golden-morning-25</a></strong> to <a href=\"https://wandb.ai/jefsnacker/surnamerator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jef/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 2.7 M \n",
      "-------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.749    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904bbd870e7d4f7fa01865e856c6c9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jef/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/jef/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad441a794ae423f9180d219230cc5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LitSurnames(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.data = NameData('compiled_names.txt', self.config[\"window\"])\n",
    "        self.config[\"vocab\"] = self.data.vocab()\n",
    "\n",
    "        self.model = GPT(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        return F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch)\n",
    "        self.log('tr_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.loss(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.model.configure_opt()\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.data.train_data_loader(), \n",
    "            batch_size=self.config[\"batch_size\"],\n",
    "            # num_workers = config[\"num_workers\"]\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data.test_data_loader(), batch_size=self.config[\"batch_size\"])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data.val_data_loader(), batch_size=self.config[\"batch_size\"])\n",
    "\n",
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    run = wandb.init(project=\"surnamerator\", reinit=True)\n",
    "    logger = WandbLogger()\n",
    "else:\n",
    "    import os\n",
    "    logger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "lit_surname = LitSurnames(config)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    logger=logger,\n",
    "    callbacks=[lr_monitor]\n",
    ")\n",
    "\n",
    "trainer.fit(lit_surname)\n",
    "\n",
    "if use_wandb:\n",
    "    run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18e189b7a455dca94c157be0e0713a038d2605132cba86745af50b7dbf438d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
