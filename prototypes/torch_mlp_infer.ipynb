{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2f917f-e5e5-42e7-abf3-747686a2fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4145b277-d138-4929-865e-d9737660286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"torch_mlp_config.yaml\")\n",
    "\n",
    "mlp_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"mlp_weights.pt\")\n",
    "\n",
    "wavenet_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"wavenet_config.yaml\")\n",
    "\n",
    "wavenet_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"wavenet_weights.pt\")\n",
    "\n",
    "gpt_nano_config_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"gpt_config.yaml\")\n",
    "\n",
    "gpt_nano_weights_path = huggingface_hub.hf_hub_download(\n",
    "    \"jefsnacker/surname_generator\",\n",
    "    \"gpt_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ece47c72-08d8-4496-841e-23ca8a023d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mlp_config_path, 'r') as file:\n",
    "    mlp_config = yaml.safe_load(file)\n",
    "\n",
    "with open(wavenet_config_path, 'r') as file:\n",
    "    wavenet_config = yaml.safe_load(file)\n",
    "    \n",
    "with open(gpt_nano_config_path, 'r') as file:\n",
    "    gpt_nano_config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6733075-1c64-40a3-82be-7940bc6842ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (first): Linear(in_features=30, out_features=200, bias=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (4): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (7): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Tanh()\n",
       "    (9): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (10): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Tanh()\n",
       "    (12): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (13): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Tanh()\n",
       "    (15): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (16): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): Tanh()\n",
       "    (18): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (19): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): Tanh()\n",
       "    (21): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (22): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): Tanh()\n",
       "    (24): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (25): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): Tanh()\n",
       "    (27): Linear(in_features=200, out_features=200, bias=False)\n",
       "    (28): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): Tanh()\n",
       "  )\n",
       "  (final): Linear(in_features=200, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_char, hidden_nodes, embeddings, window, num_layers):   \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.window = window\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.C = nn.Parameter(torch.randn((num_char, embeddings)) * 0.1, requires_grad=True)\n",
    "        \n",
    "        self.first = nn.Linear(embeddings*window, hidden_nodes)\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.layers = self.layers.extend(nn.Sequential(\n",
    "                nn.Linear(hidden_nodes, hidden_nodes, bias=False),\n",
    "                nn.BatchNorm1d(hidden_nodes),\n",
    "                nn.Tanh()))\n",
    "\n",
    "        self.final = nn.Linear(hidden_nodes, num_char)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.C[x]\n",
    "        x = self.first(x.view(-1, self.window*self.embeddings))\n",
    "        \n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "    \n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "     \n",
    "mlp = MLP(mlp_config['num_char'], \n",
    "          mlp_config['hidden_nodes'], \n",
    "          mlp_config['embeddings'], \n",
    "          mlp_config['window'], \n",
    "          mlp_config['num_layers'])\n",
    "\n",
    "mlp.load_state_dict(torch.load(mlp_weights_path))\n",
    "mlp.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "077f61d9-3c96-4624-8187-8daf8255add1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WaveNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Embedding(30, 10)\n",
       "    (1): Conv1d(8, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Tanh()\n",
       "    (4): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Tanh()\n",
       "    (7): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (8): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Tanh()\n",
       "    (10): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (11): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Tanh()\n",
       "    (13): Conv1d(100, 100, kernel_size=(2,), stride=(1,), bias=False)\n",
       "    (14): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Tanh()\n",
       "    (16): Flatten(start_dim=1, end_dim=-1)\n",
       "    (17): Linear(in_features=500, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, num_char, hidden_nodes, embeddings, window, num_layers):   \n",
    "        super(WaveNet, self).__init__()\n",
    "        \n",
    "        self.window = window\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.embeddings = embeddings        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Embedding(num_char, embeddings)\n",
    "        )\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                nodes = window\n",
    "            else:\n",
    "                nodes = hidden_nodes\n",
    "                \n",
    "            self.layers = self.layers.extend(nn.Sequential(\n",
    "                nn.Conv1d(nodes, hidden_nodes, kernel_size=2, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(hidden_nodes),\n",
    "                nn.Tanh()))\n",
    "            \n",
    "        self.layers = self.layers.extend(nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_nodes*(embeddings-num_layers), num_char)\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "    \n",
    "wavenet = WaveNet(wavenet_config['num_char'], \n",
    "                  wavenet_config['hidden_nodes'], \n",
    "                  wavenet_config['embeddings'], \n",
    "                  wavenet_config['window'], \n",
    "                  wavenet_config['num_layers'])\n",
    "wavenet.load_state_dict(torch.load(wavenet_weights_path))\n",
    "wavenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9646eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (vocab_emb): Embedding(30, 48)\n",
       "  (pos_emb): Embedding(32, 48)\n",
       "  (emb_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=48, out_features=144, bias=True)\n",
       "        (head): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=48, out_features=192, bias=True)\n",
       "        (l2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=48, out_features=144, bias=True)\n",
       "        (head): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=48, out_features=192, bias=True)\n",
       "        (l2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (attn): GptAttention(\n",
       "        (w_attn): Linear(in_features=48, out_features=144, bias=True)\n",
       "        (head): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=48, out_features=192, bias=True)\n",
       "        (l2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head_layer_norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=48, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class GptAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    For this attention module k = v = q are all the same.\n",
    "    It's for encoder/decoder only transfomers.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(GptAttention, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert self.config[\"d_model\"] % self.config[\"heads\"] == 0\n",
    "        self.heads = self.config[\"heads\"]\n",
    "\n",
    "        self.w_attn = nn.Linear(self.config[\"d_model\"], 3*self.config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"d_model\"])\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config[\"attn_pdrop\"])\n",
    "        self.resid_dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\", \n",
    "            torch.tril(\n",
    "                torch.ones(\n",
    "                    self.config[\"window\"], \n",
    "                    self.config[\"window\"])\n",
    "                ).view(1, 1, self.config[\"window\"], self.config[\"window\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, window, embs = x.shape\n",
    "\n",
    "        q, v, k = self.w_attn(x).split(self.config[\"d_model\"], dim=2)\n",
    "\n",
    "        # (B, heads, window, embs)\n",
    "        q = q.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        k = k.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        v = v.view(\n",
    "            B, \n",
    "            window, \n",
    "            self.config[\"heads\"], \n",
    "            embs // self.config[\"heads\"]\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # Self-attend: (B, heads, window, embs) x (B, heads, embs, window) -> (B, heads, window, window)\n",
    "        scores = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        mask = scores.masked_fill(self.bias[:,:,:window,:window] == 0, float('-inf'))\n",
    "        probs = F.softmax(mask, dim=-1)\n",
    "        attn = self.attn_dropout(probs)\n",
    "        attn = probs @ v\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, window, embs)\n",
    "\n",
    "        return self.resid_dropout(self.head(attn))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(config[\"d_model\"], 4*config[\"d_model\"])\n",
    "        self.l2 = nn.Linear(4*config[\"d_model\"], config[\"d_model\"])\n",
    "        self.dropout = nn.Dropout(config[\"resid_pdrop\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = NewGELU()(self.l1(x))\n",
    "        return self.dropout(self.l2(x))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.attn = GptAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm2 = nn.LayerNorm(config[\"d_model\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        x = self.norm2(x + self.ff(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.vocab_emb = nn.Embedding(self.config[\"vocab\"], self.config[\"d_model\"])\n",
    "        self.pos_emb = nn.Embedding(self.config[\"window\"], self.config[\"d_model\"])\n",
    "        self.emb_dropout = nn.Dropout(config[\"embd_pdrop\"])\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(self.config) for _ in range(self.config[\"blocks\"])])\n",
    "        self.head_layer_norm = nn.LayerNorm(config[\"d_model\"])\n",
    "        self.head = nn.Linear(self.config[\"d_model\"], self.config[\"vocab\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        vocab_emb = self.vocab_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(0, x.shape[1], dtype=torch.long, device=x.device))\n",
    "\n",
    "        x = self.emb_dropout(vocab_emb + pos_emb)\n",
    "\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "\n",
    "        x = self.head_layer_norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_opt(self):\n",
    "        p_decay = set()\n",
    "        p_no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    p_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    p_no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = p_decay & p_no_decay\n",
    "        union_params = p_decay | p_no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_decay))], \"weight_decay\": self.config[\"weight_decay\"]},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(p_no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, \n",
    "            lr=self.config[\"lr\"], \n",
    "            betas=(self.config[\"b1\"], self.config[\"b2\"])\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def sample_char(self, x):\n",
    "        logits = self(x)\n",
    "        probs = F.softmax(logits[:,-1,:], dim=1)\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "gpt_nano = GPT(gpt_nano_config)\n",
    "gpt_nano.load_state_dict(torch.load(gpt_nano_weights_path))\n",
    "gpt_nano.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5849cbd4-b292-49ba-901e-0a0d18332b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " device(type='cpu'),\n",
       " Parameter containing:\n",
       " tensor([[-0.5183,  0.0326,  1.4677,  ...,  1.1205,  0.8975,  1.3533],\n",
       "         [ 0.8270, -1.1276,  1.2416,  ...,  2.4547,  1.7785,  2.9467],\n",
       "         [ 0.0877,  0.1408,  0.9961,  ...,  0.0688,  0.2418,  0.8137],\n",
       "         ...,\n",
       "         [ 0.3698, -0.7645,  0.8274,  ...,  0.3665, -0.1461,  0.3645],\n",
       "         [ 1.1430, -1.0511,  1.2755,  ...,  0.3180, -0.1909, -0.3718],\n",
       "         [-0.9094, -1.5168,  1.5178,  ...,  0.5179,  0.5387,  1.8614]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(mlp.parameters()).device, next(wavenet.parameters()).device, next(gpt.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b2bff2e-d15b-451e-92f4-4a9778084193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_names(name_start, number_of_names, model):\n",
    "    if model == \"MLP\":\n",
    "        stoi = mlp_config['stoi']\n",
    "        window = mlp_config['window']\n",
    "    elif model == \"WaveNet\":\n",
    "        stoi = wavenet_config['stoi']\n",
    "        window = wavenet_config['window']\n",
    "    elif model == \"GPT Nano\":\n",
    "        stoi = gpt_nano_config['stoi']\n",
    "        window = gpt_nano_config['window']\n",
    "    itos = {s:i for i,s in stoi.items()}\n",
    "\n",
    "    names = \"\"\n",
    "    for _ in range((int)(number_of_names)):\n",
    "    \n",
    "        # Initialize name with user input\n",
    "        name = \"\"\n",
    "        context = [0] * window\n",
    "        for c in name_start.lower():\n",
    "            name += c\n",
    "            context = context[1:] + [stoi[c]]\n",
    "\n",
    "        # Run inference to finish off the name\n",
    "        while True:\n",
    "            x = torch.tensor(context).view(1, -1)\n",
    "            if model == \"MLP\":\n",
    "                ix = mlp.sample_char(x)\n",
    "            elif model == \"WaveNet\":\n",
    "                ix = wavenet.sample_char(x)\n",
    "            elif model == \"GPT Nano\":\n",
    "                ix = gpt_nano.sample_char(x)\n",
    "                \n",
    "            context = context[1:] + [ix]\n",
    "            name += itos[ix]\n",
    "                \n",
    "            if ix == 0:\n",
    "                break\n",
    "            \n",
    "        names += name + \"\\n\"\n",
    "        \n",
    "    return names\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_names,\n",
    "    inputs=[\n",
    "        gr.Textbox(placeholder=\"Start name with...\"),\n",
    "        gr.Number(value=5),\n",
    "        gr.Dropdown([\"MLP\", \"WaveNet\", \"GPT Nano\"], value=\"GPT Nano\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f2967-f5ea-46e4-b76b-c1eeb9007725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "18e189b7a455dca94c157be0e0713a038d2605132cba86745af50b7dbf438d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
